name: Run autoshift scraper

on:
  schedule:
    # every 40m
    - cron: '40 * * * *'
  workflow_dispatch: {}

concurrency:
  group: autoshift-scraper
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Ensure data/shiftcodes.json is present for scraper
        run: |
          # create data dir the scraper expects
          mkdir -p data
          # prefer the file checked out in the repo root (if present)
          if [ -f shiftcodes.json ]; then
            cp shiftcodes.json data/shiftcodes.json
            echo "Copied existing shiftcodes.json from repo root to data/shiftcodes.json"
          else
            # attempt to download the file from the repo's main branch raw URL
            RAW_URL="https://raw.githubusercontent.com/${{ github.repository }}/main/shiftcodes.json"
            echo "shiftcodes.json not present in checkout; attempting to download from $RAW_URL"
            if curl -fsSL "$RAW_URL" -o data/shiftcodes.json; then
              echo "Downloaded shiftcodes.json to data/shiftcodes.json"
            else
              # fallback to an empty valid structure to avoid parser errors
              echo "[] " > data/shiftcodes.json
              echo "Failed to download shiftcodes.json; wrote empty array to data/shiftcodes.json"
            fi
          fi

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      # Cache pip download/wheel cache to speed installs between runs
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ./.pip-cache
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-py-3.12
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install --cache-dir ./.pip-cache -r requirements.txt; fi

      - name: Run scraper
        env:
          SCRAPER_USER: ${{ secrets.SCRAPER_USER }}
          SCRAPER_REPO: ${{ secrets.SCRAPER_REPO }}
          SCRAPER_TOKEN: ${{ secrets.SCRAPER_TOKEN }}
          SCRAPER_ARGS: ${{ secrets.SCRAPER_ARGS }}
        run: |
          python autoshift_scraper.py --user "${SCRAPER_USER}" --repo "${SCRAPER_REPO}" --token "${SCRAPER_TOKEN}" ${SCRAPER_ARGS}
